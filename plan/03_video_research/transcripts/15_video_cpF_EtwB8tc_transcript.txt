Transcript:
[Music]
If you're new here, a little bit of
context on me. My name is Chris and I
build productivity apps and I try to
document everything I'm building and
share as much as possible with you guys
on this channel. Today we're taking a
break from the main apps I usually work
on to build something just for fun. If
I'm being honest here, I am a little bit
burnt out. The app I'm focusing on right
now, Subscription Monster, has been a
bit tough recently, especially if you
saw my last video on the pricing
challenges I was facing. And while I
think I finally solved it, I think I do
need a break. So, for today, we're going
to build something just for fun, just to
learn two things that I've been curious
about. The first is Apple's recently
released liquid glass design, and the
second is their foundation models, which
are AI models that run locally on the
device. There is no pressure to release
this at all. It's just an experiment for
me to see where the technology is at.
So, I spent the morning thinking about
what to build so I can experiment with
this stuff. I wanted something simple,
but useful enough for me to actually
test out these technologies. And here's
the idea that I came up with. It's a
calorie tracking app, but with a twist.
Instead of searching databases or
scanning barcodes, you just type the
food you're eating like you would in
Apple Notes. So, if you say something
like an In-N-Out Burger with fries, the
AI in the background is going to try to
figure out the calories and then display
that to the right of the text that you
just typed in. I think that the Apple
Foundation models would be perfect for
this. They're free. They run locally and
are completely private and I think they
should be good for simple calorie
estimation. Once I figured out what I
wanted to build, I headed to the gym
with Cecilia. But if I'm being honest, I
really was not focused on the workout
because I could not stop thinking about
this app. In between sets, I was reading
articles about Liquid Glass. I'm
probably one of the only people that
goes to the gym and reads documentation
between my sets. But we did get the
workout done and then we did some cardio
to wrap it up. The cardio went by super
quick because I spent the entire 30
minutes looking at design inspiration
and just planning out what I was going
to build. And the second I got home, I
wasted literally no time and immediately
started working on the app. So, I made
some real progress here. We got the
basic UI working. Let me show you guys.
Probably a little hard to see because of
the brightness, but when you click it,
it is liquid glass. There's also this
settings button here, which is also
liquid glass. And then this settings
page also uses liquid glass. Something I
really like is that at the top, as you
scroll, you can see this gradient fade
type situation going on. I absolutely
love this. I'm 100% porting this over to
my other apps. settings page is not
actually functional. This is oneshotted
by Claude Code. Oh, but this daily
calorie thing is functional. So, you can
set the daily calories here and it will
show up at the bottom here. And this is
a liquid glass animation where you can
have elements coming out of each other.
It's kind of hard to see, I think, with
the white on white, but it is really
good. There are haptics for this. And
then this is hardcoded right now, but
this also does function as well. Burger.
It's going to calculate and it's going
to show the calories on the right. So,
if I type in something like egg, it's
going to say calculating and then it's
going to show some calories on the
right. Okay, it seems like it's kind of
broken here, but this is what it's
supposed to be doing. And I even have
this really nice animation here. Yeah,
really proud of that. And then we have
this logo here, which is Amy the Cat
stretching. I know it's kind of dumb
because this is just a demo app that's
going to be thrown away, but I really
wanted to add some branding so it feels
a little bit more real and I don't know,
it's just a little bit more enjoyable to
use. Something interesting is that I'm
using clawed code in here. It's actually
not that great at Apple's liquid glass.
But then when I use Xcode and I'm using
the new intelligence feature, which this
is basically Apple's version of AI
coding, this thing is surprisingly well
trained at liquid glass. Actually, I
guess it's not that surprising because
it is Apple's own AI and I would expect
it to be trained on the latest
documentation. Let me show you guys what
this looks like. So, if I say how does
liquid glass work, this is actually
using my Claude account. So, if you
weren't aware, you can actually hook up
your Cloud account and it'll use your
subscription to power Apple
intelligence. And you can use a bunch of
different providers. I think OpenAI has
a provider as well. But this is pretty
good. I didn't even have to feed in the
documentation here. It just
automatically knows what liquid glass
is. This is something that Claude Code
was struggling with. Anytime I asked it,
actually, let's just try it here. What
is liquid glass in iOS 26? Now, it wants
to search the web, which is cheating. I
want to see if it knows what this is. In
its training set, which ends January
2025, liquid glass on iOS 26 did not
exist yet. So, it has no concept of
this. And I have noticed that even if I
do feed in the Apple documentation or
use the contact 7 MCP, it just does not
understand this like Xcode intelligence
does. So, that was kind of interesting
to see. I've never actually used Xcode
intelligence before. If you need to use
the latest Swift and Apple
documentation, I think this is actually
a pretty good use case. After the quick
coding session at home, I headed to a
co-working space that I recently got a
membership for. It's called Switchards,
and they recently came to Dallas. To be
honest, I'm actually usually not working
from home. I'm usually at a coffee shop
or this co-working space now. But I love
working from here because it's so
aesthetic, and everyone in here is just
heads down doing work, and I feel like I
have to do the same thing, too. With the
UI in place with the liquid glass, it
was now time to try to hook in the Apple
Foundation models to actually do the
calorie estimation. I came in really
optimistic about this because I thought
that the models would be pretty well
trained on this type of data. Is this
Oops. Is that better?
Is that better? I think it was better.
So, I got Apple's foundation model
hooked up. It was honestly pretty easy.
It took about 15 minutes. But the issue
is it is a little bit inaccurate. For
example, if I type In-N-Out Burger,
which according to the In-N-Out website
is about 300 something calories, the
Apple Foundation model says that it's
around 840 calories. I'm not that
surprised because I don't think these
models were really trained for this
purpose. I think Apple's models are more
for summarization and small textbased
tasks. Since that was so easy, I decided
to try some other local models just to
see how they stack up against Apple's
foundation models. What I decided to do
was use a library called llm.sswift,
which lets me download local models that
could be used on device from another
service called HuggingFace, which hosts
these local models. And these are all
open- source models, completely free to
use. They run offline. They're private,
similar to Apple's Foundation models.
The models though are a bit bigger,
which hopefully means that they're even
better at this task, but most of them
are not optimized to run on such a small
device. So, that is the pro of Apple's
models. They are fully optimized for the
iPhone. And these local models I'm about
to test are not. The two models I
decided to try were Meta's Llama model
and Google's GMA model. GMA, or is it
Gemma? I actually don't know. GMA or
Gemma. This is the other one I decided
to try. And this one was actually
optimized to run on mobile. So, I was
really curious to see how this one would
perform. Don't know if you guys can see
this. This is actually the code for me
to switch between these two different
models. And I'm using the llm.sswift
package. And I'm basically downloading
the model if it doesn't exist. And then
I actually call some functions in here
to generate a response and actually call
the model. And I built this really nice
Oh, let me just take this off.
And I built this really nice switcher
where you can actually choose between
Apple's foundation model or the llama or
the GMA model. And then you can just
download it on the device if it's not
already there. So, let me actually go
download. Yeah, the GMA model is
actually really big. It's almost 2 GB.
So, I'm going to go ahead and download
this. And this is what this looks like.
And then once it's done, I'll be able to
actually use it. So, I tried both of
these models out. And the accuracy did
improve just a little bit, but there was
a significant hit to performance. The
Apple Foundation models just ran so much
smoother on device. The accuracy gains
were not high enough to justify
switching over from the Foundation
model. I think that the models are
trained on nutrition data, but honestly,
I don't think that was a huge priority
to the model providers. So, it makes
sense why they're not performing that
well. So, the next thing I'm going to
try is using third-party cloud-based
providers like OpenAI, Anthropic, these
bigger models that have to be hosted on
an external server. Before we get into
it, I do want to mention that the way
I'm building this is by using Cloud
Code, and I'm just dictating everything
into Cloud Code, telling it exactly what
I want. And if you've seen any of my
videos, you know that that's my
recommendation when you're using
something like Cloud Code to prototype
quickly. It's to dictate everything
because you get way more detailed
prompts than if you just typed
everything manually. And I get a lot of
questions on what tool do I use for
dictation and the answer is Whisper
Flow. Huge shout out to them for
actually being a channel sponsor.
Whisperflow is smart voice to text that
works in every single app. The reason
that I use Whislow is because it
understands developer terminology really
well. So when I say something like use
state, convex schema, web hook handler,
it gets it right every single time. And
I use Whisflow outside of coding, too.
I'm constantly dictating emails, sending
text messages with their iOS app, and
dictating directly into Claude. I
literally write all my YouTube scripts
by just dictating my thoughts into
Claude desktop. It saves me so much time
because typing everything by hand would
take me like 30 minutes. It's available
on the desktop, so you can use it inside
of Cursor, Claude, or Claude Code like I
do, but you can also use it on iOS as
well. I'll leave a link in the
description. I also have a code that
gives you 1 month free. Technically, you
can use FlowBasic free forever. And you
already get a 14-day trial of Pro when
you sign up, but this is an additional 1
month on top of that if you use my code.
They did not have to do that. I'm the
one that asked them. So, huge thank you
to them for providing that. But feel
free to check them out if you're
interested. Okay, so back to the
problem. I've decided, let's try not
using local models. Let's try out these
cloud-based providers because they're
larger. They work on the server. Maybe
they'll be more accurate because they
could be better trained on nutrition
data.
[Music]
Um, so I tried the cloud models. They
were definitely a bit better. Even
something like the Gemini 2.5/ Light
model was a bit more trained on calorie
data. I think it got generic things like
if I say burger or pad thai, it did seem
like it was accurate enough, but it was
still failing things like if I put
In-N-Out Burger, for example, it was not
using the calorie data that you could
find on their website, which gave me an
idea. What if I could just feed a
nutrition database directly into one of
these models. So I looked up some
nutrition database APIs and there was
one that stood out called Fat Secret.
And here's how I got it to work within
the app. I'm actually using a small
lightweight model which is the Gemini
2.5 flash light model to first extract
the food and the portions directly from
what the user is typing. And then I hit
the Fat Secret API to get the exact
nutrition data. That nutrition data is
sent back to Gemini so it has all the
information it needs. And then it does
all of the math and calculations
depending on what portions are typed
into the app. And if you look at the
console, this is exactly what's going
on. So it's breaking down In-N-Out
Burger and fries into components,
fetching the nutrition data API from the
Fat Secret API, and then it's
calculating everything based on the
portion size. And it is way more
accurate than relying on the training
data set that comes with the base model.
So if I say something like an In-N-Out
Burger, it gets it. And then if I say
half an In-N-Out Burger, it gets that.
Now, the downside with this though is
that the Fat Secret API is extremely
expensive. I reached out to the team and
to use this API commercially, it's at
minimum $1,500 a month. There is a free
tier that you can use, but you have to
put their branding on almost every
single page, which I would rather not do
if possible because I really love the
minimal aesthetic that the app already
has. But this is not a bad solution to
the accuracy problem. Way more accurate
than the local models, which makes
sense. To be honest, this is a pretty
good stopping point. We tested
everything we needed with Apple Glass
and the foundation models, but I really
feel like there's a better solution
here. So, I'm going to take a break
right now, brainstorm a couple more
solutions, and just see if we come up
with anything. While I was grabbing
dinner, I was still thinking about this
API problem. The cost was just a little
bit too high, and I really did not want
to show the logo in the app. To be
honest, this wasn't a dealbreaker
because the whole point of this project
was for me to just understand the
foundation models, which I already did,
but I really did want to see if there
was a better way to do this. The
co-working space is actually open 24
hours. So after dinner, headed back to
the co-working space since it was like 5
minutes away just to iterate and see if
there was a better way to do the calorie
estimation. I had a little bit of a
breakthrough here. I was doing a little
bit of research trying to figure out
what is the best model that is probably
trained on calorie data because clearly
the Apple Foundation models aren't that
good, but I have noticed that other
things like the Gemini models or the
claude models, they are a little bit
more trained at this stuff. So I
thought, is there a model that has this
information kind of baked in? And I
actually did find a model that was
really good at it and it's called
Perplexity Sonar. What's special about
this model is off the bat it comes with
search engine capabilities. When you
call it, it also will run a search on
the internet. So if I type in what are
the calories of an In-N-Out Burger, it's
going to look through all of these
sources and it's going to take all the
information and try to answer the
question. Happens to be really good at
answering these questions about calorie
and nutrition information. Now, instead
of just being tied to the Fat Secret
API, through the Sonar API, I'm able to
use a bunch of these nutrition databases
and even the restaurant websites
themselves. And just from using it for a
few minutes and testing it a bit, it is
really, really accurate and it's really
smart, too. So, it's able to handle
things like half of an In-N-Out Burger.
I'd heard about the Sonar model, but I
really have not had a good use case for
it. This is a very good use case for
this model. Now, to be honest, the real
negatives here though are the pricing.
I'm using so few tokens that that
doesn't even matter to be honest. The
thing that matters is that every time
you call the Sonar API, they do charge
you for the search engine requests. And
the pricing right now is $5 for
every,000 requests, which is way more
expensive than I'm paying for the Gemini
2.5 flash light calls. So, this is a bit
of a pricey model even though I'm not
using that many tokens. But I genuinely
think for the quality that I'm getting
here, it is totally worth it. So, if I
do release this app, I will probably be
using the Sonar API. It also has this
really cool benefit where it shows me
the sources that were used. So in the
UI, I can show this really cool thing
where when I show the nutrition info
below it, I can show these were the
sources I used to get the nutrition
data. When I saw that on the UI, that
was a big magic moment for me because it
got me to trust the data a little bit
more, which is a problem that I see with
a lot of these calorie tracking apps.
I'm really happy with how this turned
out. The app's actually way cooler than
I thought. I'm personally going to be
using this because I do want to start
tracking calories and getting my diet in
order and this is just a really cool app
to use for this. So, this experiment to
learn liquid glass and the Apple
foundation models actually ended with a
pretty good app. It's really unfortunate
that the foundation models did not work,
but it was really cool to learn how easy
it was to use and what the limitations
of the model are. Now, I was not
planning on releasing this app. I
already have a bunch of other apps I'm
working on. Subscription Monster hasn't
even been released yet, but I have
decided to throw up a weight list just
to gauge, do people actually want this?
and I actually made a post on Twitter
talking about the app and to my surprise
it did actually blow up blow up relative
to my other tweets. It didn't really go
viral or anything, but there was a lot
of interest in this. So, I did throw up
the wait list. So, if you want to join
that, I will leave a link in the
description below. The app is actually
pretty simple and almost done. So, I'm
going to try to release the beta a week
after this video goes live. Honestly,
this is exactly what I needed. I started
the day pretty burnt out because I'm
building a complex app like Subscription
Monster and I ended the day by building
another complex app. But that's okay
because this was a really fun project to
work on and I learned a lot in the
process. I had a lot of fun making this
video. If you did like it, please let me
know in the comments because I can
totally make more videos like this. If
you like this kind of content, check out
my Instagram and Tik Tok. I post almost
every other day about building
productivity apps. And obviously, if you
like this one, don't forget to
subscribe. But thank you guys so much
for spending the day with me and I will
see you guys in the next video.
[Music]
