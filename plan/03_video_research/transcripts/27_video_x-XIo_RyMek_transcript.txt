Transcript:
It's been about a month since I
launched. Oops. [music]
Forgot my duck. I have so many of these.
For anyone who hates that I hold the
duck, put them all right here. Okay.
No, no, I can't do that. You're going to
have to go over here. It's been about a
month since I launched my app and we
just hit a huge milestone. We have a 100
paying users and that's about $1,000 in
monthly recurring revenue. This is by
far the fastest any of my apps have
grown. It's been crazy to see. And if
you want to see all the past work to get
here, check out the past videos below.
But we're just getting started and
there's still a ton of work to do. In
this video, I'm going to show you all
the things that I've been building to
keep that momentum going. And I'm going
to be honest about the biggest problem
that I'm facing right now. If you're new
here, welcome to the video. My name is
Chris and I build productivity apps. I
usually focus on one productivity app
per video and today we're focusing on my
newest app, Amy. Quick context. Amy is a
calorie tracking app in the style of
Apple Notes. You just type what you ate
on the left and on the right the
calories will magically appear. I've
been working on this app for about 2 and
a half months now and I've shared
everything on this channel. In the last
video, we talked about the tweaks I made
to the payw wall, the onboarding, all
the things to try to get people to start
a trial. But to be honest, that's only a
fraction of the problem. The bigger
issue is retention, which really means
how many people after signing up are
sticking with the app one week later.
That's the number that I care about. And
to be honest, it's not looking great.
We're sitting at about an 8% week 1
retention rate, which means that if a
100 people start a trial, only eight of
them are sticking around 1 week later.
So, that's the biggest problem I'm
trying to solve right now. No amount of
marketing is going to fix this because
if I can't get people to stick with the
app, there's no point in getting these
downloads. So, how do you fix this?
Unfortunately, there's no secret trick.
You really just have to talk to users,
figure out why they're leaving, and then
build things to make sure that they
stay. For me, the best places of
feedback are a feedback board that I
have set up so people can leave feedback
and then upvote the things that they
want to see in the app. Feedback from
comments and DMs on social media because
I'm posting about this everywhere. So,
there's a lot of feedback rolling in.
And then people also reach out directly
through email. These last two channels
are very important because if someone is
taking the time to give you direct
feedback [music]
that means that this is a problem that's
very important to them. So I take it
very seriously. I was looking at all the
requests trying to figure out okay what
can I build to try to move the needle on
retention. And the first feature I
started working on which kept popping up
even during the development of the app
is can you add the ability to log food
by taking a picture. To be honest I have
been seriously avoiding building this
feature because there are thousands of
calorie tracking apps that are take a
photo of your food and we're going to
scan it with AI and get the calories. I
really didn't want people to think that
it was just another one of those apps.
But the feedback has kept coming up over
and over again and I just could not
ignore it anymore. And to be honest,
after using the app for a bit, it makes
a lot of sense. Even myself, I'm
noticing when I'm in a restaurant, there
are instances where it would be really
nice if I can just take a picture of the
food right there. Especially if it's
something that has a lot of ingredients
that are visible in a photo. So, I built
a quick prototype. It took about an hour
to build. And I was honestly very
surprised at how good it came out. So
now when you open the app, there is a
new button that signals you can take a
picture of your food. When you click it,
it's going to ask you if you want to
take a photo, do you want to upload a
photo? It's actually pretty simple.
We're going to take the photo and try to
make a very accurate description of what
we see. And then we're going to populate
it into Amy as if the user typed it. And
it actually is pretty good. Like check
this out. When I took a picture of this
sashimi platter, it correctly identified
how many pieces of sashimi and what type
of sashimi it was. I even fed in this
picture, which I was pretty confident it
would not be able to get. It was a
halfeaten dish and I was so surprised
that it got it correctly. It even
identified and broke down the
ingredients for me. And here was a
really cool example of me using it. It
actually identified that I was given the
wrong order. I am not making this up. I
ordered a chicken sandwich at a
restaurant and when I took a photo, it
identified it as tuna and I was like,
"Okay, well, I guess it got it wrong."
Then I started eating it and realized,
"Wait a minute, this is actually tuna."
The restaurant gave me the wrong order.
So that was kind of crazy that it
correctly identified that even when me
looking at it, I was pretty sure that
this was chicken. I mean, in hindsight,
it didn't actually look like chicken,
but I thought it was. So that was a
really cool magic moment where it
actually was better than me at
identifying the food. And the AI model
I'm using to identify the food is Gemini
2.5 Flash Light, which is probably going
to be surprising to people. It's
actually very accurate, and more
importantly, it's super fast and cheap.
Each photo I'm uploading is costing me
like a fraction of [music] a penny. I
thought this was going to be pretty
expensive to run, but this is actually
pretty doable. I think a reason, too, is
that a lot of the heavy lifting is still
being done by Perplexity Sonar. So, when
we take a photo, Gemini 2.5 flashlight
is going to identify what's in the
photo, but it's not going to do anything
else. It's just going to make a
description for us. Then, we're sending
this to Proplexity Sonar, which is going
to do all the heavy lifting, do the
research, find the calories, and all the
cost is going to be there. And I built
it in a way where you can edit the
description. So, if Gemini got it wrong,
you can make a quick alteration and say,
"Hey, it is actually chicken. It's not
tuna." or you can edit a portion size or
an ingredient that it got wrong.
Optionally, you can also say what
restaurant it's from, and we will use
this in the Perplexity Sonar call to try
to get more accurate data from the
restaurants themselves. I've been using
this constantly over the last week, and
it is probably one of my favorite ways
to input food now. And more importantly,
in some cases, it is way more accurate
than me typing it. I still type a lot of
things, especially when it's not in
front of me, or if it's a simple
restaurant, like an In-N-Out Burger with
fries, I can just type that instead of
taking the photo. But when there's a lot
going on with the meal, it's so much
easier to just snap the photo. My hope
is that this makes the app easier. It
reduces friction, which again is going
to hopefully reduce churn and improve
our retention numbers. I'll be honest, I
have a lot of regret here. This is one
of those features that I kind of wish I
shipped on day one. The next thing
people kept asking for was the ability
to add micronutrients to the app. Right
now, we can track macronutrients. So,
that's protein, carbs, and fats. But a
lot of people requesting the ability to
track things like sugar and fiber. The
thing is, almost every other calorie
tracking app on Earth supports this. So,
if my app doesn't have it and they see
that, they're immediately just not going
to use it. Adding this took a little bit
longer than I expected because it just
touches so much of the app. But now
there's a new section in the settings
where you can enable micronutrients and
you can set targets for each one. And
you don't have to enable them all. Like
if you only want to track sugar, you can
just turn on sugar tracking. More
importantly, I added this to the
onboarding so it's very clear to users
that Amy does support this feature. So,
here's something interesting that
happened when I enabled this. I learned
very quickly that I am consuming a lot
of sugar. I drink a bunch of lattes and
matcha things and I was not aware of how
much sugar is in some of these things,
which was a really cool moment for me
because that means that the app taught
me something about my own nutrition that
I was not aware of. It was a magic
moment. And when you're building an app,
these magic moments that surprise users,
those are the moments that you want to
try to recreate. Now, I'm interested in
adding more features like this that are
educational to show users this is how
much sugar you should be consuming. This
is how much fiber you should be
consuming. But what originally started
as a feature to just make sure I don't
lose users actually ended up opening the
door to a bunch of new features that I'm
probably going to add. These kind of
moments build trust and trust is what
gets users to stick around with your
app. By the way, the photo tracking
feature and the micronutrients [music]
were very easy to ship. I think it took
like less than a day to ship both of
these combined. And it's because I'm
using tools like cloud code and cursor
to move 10 times faster. If you look at
my prompts, they're pretty detailed. And
the way I achieve that is by dictating
everything. [music] I get a lot of
questions asking what am I using for
dictation and the answer is whisperflow
and a huge shout out to them for being a
channel sponsor. [music] Whisperflow is
smart voice to text that works in any
app. The reason whisperflow is perfect
for developers is it understands
technical terminology. So when I say
something like ustate [music]
convex schema web hook handler
it gets it right every single time. And
if you're using it with Cursor,
Windsurf, Whisper Flow has integrations
with these IDEs and it can actually
understand what's going on in the code
you're looking at and tag files. So
check this out. If I'm inside Cursor and
I say, "Please enhance the styles from
subscription-overview.tsx,
[music]
it will actually tag the file without me
having to type anything on the keyboard.
It's available on desktop. So if you're
using it inside of Cursor, Claude Code,
or ChatgPT like I do, it's there. But
you can also use it on iOS as well. I
use it all the time, especially when I'm
responding to messages on iMessage or
Slack. I'll leave a link in the
description. There is a code for 1 month
free and that's on top of the 14 days
you already get of FlowPro when you sign
up. They did not have to do that. I'm
the one that asked for the discount
code. So, a huge thank you to them for
providing that. So, back to the changes
that I was making to improve retention.
This last one is a bit more of a
behind-the-scenes improvement, but I
think it really does matter for the user
experience and it's improvements I made
to the actual calorie calculation of the
app. Let me explain how it originally
worked. I'm using one AI model for
everything and that's Perplexity Sonar.
So whenever you add food, it's going to
send the request to Perplexity Sonar,
which is going to run a web search, pull
in different pieces of data from
different websites, and it's going to
take everything, calculate the calories,
and return it back to the user. We do
the exact same thing when a user makes
an edit. So if they edit the same exact
line, we're going to send that to
Perplexity Sonar. It's going to do all
its stuff, and then it's going to return
back updated nutrition information. The
problem is, not every edit requires a
whole expensive internet search. Let's
say you log a burrito and you're very
happy with the calories and then you
want to just change the portion size. So
you just write half of a burrito. You
would probably expect it to basically
half all the nutrition info. But
unfortunately what's happening is when
you do that, it's going to send a
request of perplexity and it could
accidentally pull completely different
sources for the burrito and you're going
to get back a number that is not half of
a burrito. It's probably going to be a
little bit off which happened to a ton
of users. I got a lot of reports about
this. It was very confusing for people
because when you tell it to give you
half of a number and it doesn't return
that, you start losing trust in its
ability to actually be accurate. So I
rebuilt the whole system and instead of
just using one perplexity sonar model,
we now use multiple models. The way it
works is now when a user edit comes in,
we actually first check what was edited
and we use a very lightweight cheap
model, Gemini 2.5 flash light to first
check what type of edit was made. Are we
just editing the portion or is it an
edit that requires a whole internet
search to do? And it's smart. So if it
sees that the change mentions half or 2x
or a couple of bytes, any natural
language, it's smart enough to detect
that that is just a portion change. And
if it's just a portion change, we're
going to send it to Gemini 2.5 Flash.
It's going to accurately change the
portion sizes, the total calories, the
little thought process section that's in
the app. All of that's going to be
updated by 2.5 Flash. It's super fast
and very predictable. Now, if it sees
that it's more than a portion change and
it needs to actually do a web search,
then it's going to call Perplexity to do
that update. This technique is super
common in AI systems where you first
classify the request with a cheap model
and then send it to another model
depending on what's being requested. But
this makes a very big difference and
there are three main benefits that I got
by doing this. The first is that it
actually does what the user is
expecting. So if they want to change the
portion, it's going to now accurately
change the portion. Number two is it is
much faster. The Plexity sonar calls do
take a few seconds to run because it's
doing a full web search. For Gemini 2.5
Flash, it'll now take less than a second
in some cases to return the response.
And then number three is it is way
cheaper for me to run. The perplexity
costs are about almost 1 cent per edit.
So that means every time someone was
making an edit that was costing me 1
cent. Gemini Flash is like a tenth of
that. So so much cheaper to make these
edits than it was before. I knew I would
have to rebuild the system eventually
because it made no sense to send
everything to Proplexity Sonar. For the
sake of speed and getting it out there,
I shipped it as is. But the time has now
come to build this multiAI system
because it was actually impacting
quality in this case. Now that it's
better at following these instructions
and it's faster, I think that's going to
build a lot of user trust, which again
is going to really impact retention. So,
those are some of the big things I
shipped in the last week to try to
improve retention. We're at 8% week 1
retention. So, I'm going to be
monitoring this number closely, and my
northstar is to try to get it to
something like [music] 30 or 40%. It's
going to take a lot of work, but I'm
very excited to see what kind of impact
it has on the app. The app's already
looking pretty good, so I'm excited to
see what it's going to look like in a
couple weeks. In the background, I'm
also doing some research on UGC. That's
a channel I'm very excited to start
exploring. But while we're doing that, I
need to focus on improving retention
because again, if we get the downloads,
there's no point if we [music] can't get
the users to stick with the app. So, I'm
going to keep talking to users, iterate
based on their feedback, and hopefully
we can bump that number up. Hopefully,
this was interesting. If you like this
kind of content, check out my Instagram
and Tik Tok. I post almost every other
day about building productivity apps.
And obviously, if you like this content,
don't forget to subscribe. [music] But
thank you guys so much for watching, and
I will see you guys in the next video.
>> [music]
